apiVersion: morphling.alibaba.com/v1
kind: MorphlingJob
metadata:
  generateName: gpt2-
  namespace: default
spec:
  originPerf: 3.097653527452955
  application: gpt2
  localBsz: 8
  template:
    spec:
      containers:
        - command: ["/bin/sh", "-c"]
          args: [
              "python -m torch.distributed.launch --nproc_per_node=$NPROC --nnodes=$NNODES  --node_rank=$RANK --master_addr=$MASTER_ADDR --master_port=$MASTER_PORT  /mnt/nasdata/megatron/pretrain_gpt.py     --tensor-model-parallel-size $NGPUS     --num-layers 19   --hidden-size 1920     --num-attention-heads 16     --micro-batch-size $BATCH_SIZE     --global-batch-size 8     --seq-length 1024     --max-position-embeddings 1024     --train-iters $STEPS     --lr-decay-iters 320000     --data-path /mnt/nasdata/wikidata/my-gpt2_text_document     --vocab-file /mnt/nasdata/wikidata/gpt2-vocab.json     --merge-file /mnt/nasdata/wikidata/gpt2-merges.txt     --data-impl mmap     --split 949,50,1     --distributed-backend nccl     --lr 0.00015     --lr-decay-style cosine     --min-lr 1.0e-5     --weight-decay 1e-2     --clip-grad 1.0     --lr-warmup-fraction .01     --log-interval 1     --eval-interval 100010001000     --eval-iters 1 ",
              "python -m torch.distributed.launch --nproc_per_node=$NPROC --nnodes=$NNODES  --node_rank=$RANK --master_addr=$MASTER_ADDR --master_port=$MASTER_PORT  /mnt/nasdata/megatron/pretrain_gpt.py --override-lr-scheduler  --adam-beta1 0.9   --adam-beta2 0.999     --tensor-model-parallel-size 1     --lr-decay-iters 320000     --micro-batch-size $BATCH_SIZE     --num-layers 19     --hidden-size 1920      --num-attention-heads 16     --seq-length 1024     --max-position-embeddings 1024     --train-iters $STEPS     --lr 0.00015     --lr-decay-style cosine     --min-lr 1e-5     --split 949,50,1     --log-interval 1     --eval-interval 101000100000     --eval-iters 1     --lr-warmup-fraction .01     --weight-decay 1e-2     --clip-grad 1.0 --log-optimizer-states-to-tensorboard --vocab-file /mnt/nasdata/wikidata/gpt2-vocab.json  --merge-file /mnt/nasdata/wikidata/gpt2-merges.txt  --data-path /mnt/nasdata/wikidata/my-gpt2_text_document  --data-impl mmap  --deepspeed    --zero-stage 2  --no-pipeline-parallel  --cpu-optimizer --deepspeed_config /mnt/nasdata/megatron/examples/gpt_with_pile/ds_offload_gpu${NGPUS}_mbsz${BATCH_SIZE}_gpt.json",
              "python -m torch.distributed.launch --nproc_per_node=$NPROC --nnodes=$NNODES  --node_rank=$RANK --master_addr=$MASTER_ADDR --master_port=$MASTER_PORT  /mnt/nasdata/megatron/pretrain_gpt.py --override-lr-scheduler  --adam-beta1 0.9   --adam-beta2 0.999     --tensor-model-parallel-size 1     --lr-decay-iters 320000     --micro-batch-size $BATCH_SIZE     --num-layers 19     --hidden-size 1920      --num-attention-heads 16     --seq-length 1024     --max-position-embeddings 1024     --train-iters $STEPS     --lr 0.00015     --lr-decay-style cosine     --min-lr 1e-5     --split 949,50,1     --log-interval 1     --eval-interval 100010001000     --eval-iters 1     --lr-warmup-fraction .01     --weight-decay 1e-2     --clip-grad 1.0 --log-optimizer-states-to-tensorboard --vocab-file /mnt/nasdata/wikidata/gpt2-vocab.json  --merge-file /mnt/nasdata/wikidata/gpt2-merges.txt  --data-path /mnt/nasdata/wikidata/my-gpt2_text_document  --data-impl mmap  --deepspeed    --zero-stage 2  --no-pipeline-parallel --deepspeed_config /mnt/nasdata/megatron/examples/gpt_with_pile/ds_dp_gpu${NGPUS}_mbsz${BATCH_SIZE}_gpt.json",
            ] #
          image: registry.cn-beijing.aliyuncs.com/morphling/deepspeed:mp3
          name: pytorch
          resources:
            limits:
              nvidia.com/gpu: 1
            requests:
              nvidia.com/gpu: 1
          volumeMounts:
            - mountPath: /mnt/nasdata
              name: nasdata
            - mountPath: /dev/shm
              name: mysys-shm
      imagePullSecrets:
        - name: mycred
      volumes:
        - hostPath:
            path: /data/paper/
          name: nasdata
        - emptyDir:
            medium: Memory
          name: mysys-shm
