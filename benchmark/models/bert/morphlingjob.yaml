apiVersion: morphling.alibaba.com/v1
kind: MorphlingJob
metadata:
  generateName: bert-
  namespace: default
spec:
  originPerf: 3.4885749171463454
  application: bert
  localBsz: 8
  template:
    spec:
      containers:
        - command: ["/bin/sh", "-c"]
          args:
            [
              "python -m torch.distributed.launch --nproc_per_node=$NPROC --nnodes=$NNODES  --node_rank=$RANK --master_addr=$MASTER_ADDR --master_port=$MASTER_PORT  /root/megatron/pretrain_bert.py   --tensor-model-parallel-size $NGPUS  --pipeline-model-parallel-size 1     --num-layers 24   --hidden-size 1536    --num-attention-heads 16    --micro-batch-size $BATCH_SIZE   --global-batch-size 8  --seq-length 1024   --max-position-embeddings 1024    --train-iters $STEPS    --data-path /mnt/nasdata/megatron/my-bert_text_sentence    --vocab-file /mnt/nasdata/megatron/bert-vocab.txt    --data-impl mmap    --split 949,50,1    --distributed-backend nccl    --lr 0.00015    --lr-decay-style linear    --min-lr 1.0e-5    --weight-decay 1e-2    --clip-grad 1.0    --lr-warmup-fraction .01    --log-interval 1    --eval-interval 100000000    --eval-iters 1",
              "python -m torch.distributed.launch --nproc_per_node=$NPROC --nnodes=$NNODES  --node_rank=$RANK --master_addr=$MASTER_ADDR --master_port=$MASTER_PORT  /root/megatron/pretrain_bert.py --override-lr-scheduler --adam-beta1 0.9 --adam-beta2 0.999 --tensor-model-parallel-size 1 --lr-decay-iters 320000 --micro-batch-size $BATCH_SIZE --num-layers 24 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --train-iters $STEPS --lr 0.00015 --min-lr 1e-5 --lr-decay-style linear --split 949,50,1 --log-interval 1 --eval-interval 100000000 --eval-iters 1 --weight-decay 1e-2 --clip-grad 1.0 --num-workers 4  --log-optimizer-states-to-tensorboard --vocab-file /mnt/nasdata/megatron/bert-vocab.txt --data-path /mnt/nasdata/megatron/my-bert_text_sentence --data-impl mmap --deepspeed  --zero-stage 2  --no-pipeline-parallel --cpu-optimizer --deepspeed_config /root/megatron/examples/bert_with_pile/ds_offload_gpu${NGPUS}_mbsz${BATCH_SIZE}_bert.json",
              "python -m torch.distributed.launch --nproc_per_node=$NPROC --nnodes=$NNODES  --node_rank=$RANK --master_addr=$MASTER_ADDR --master_port=$MASTER_PORT  /root/megatron/pretrain_bert.py --override-lr-scheduler --adam-beta1 0.9 --adam-beta2 0.999 --tensor-model-parallel-size 1 --lr-decay-iters 320000 --micro-batch-size $BATCH_SIZE --num-layers 24 --hidden-size 1536 --num-attention-heads 16 --seq-length 1024 --max-position-embeddings 1024 --train-iters $STEPS --lr 0.00015 --min-lr 1e-5 --lr-decay-style linear --split 949,50,1 --log-interval 1 --eval-interval 100000000 --eval-iters 1 --weight-decay 1e-2 --clip-grad 1.0 --num-workers 4  --log-optimizer-states-to-tensorboard --vocab-file /mnt/nasdata/megatron/bert-vocab.txt --data-path /mnt/nasdata/megatron/my-bert_text_sentence --data-impl mmap --deepspeed  --zero-stage 2  --no-pipeline-parallel --deepspeed_config /root/megatron/examples/bert_with_pile/ds_dp_gpu${NGPUS}_mbsz${BATCH_SIZE}_bert.json",
            ]
          image: registry.cn-beijing.aliyuncs.com/morphling/deepspeed:mp3
          name: pytorch
          resources:
            limits:
              nvidia.com/gpu: 1
            requests:
              nvidia.com/gpu: 1
          volumeMounts:
            - mountPath: /mnt/nasdata
              name: nasdata
            - mountPath: /dev/shm
              name: mysys-shm
      imagePullSecrets:
        - name: mycred
      volumes:
        - hostPath:
            path: /data/paper/
          name: nasdata
        - emptyDir:
            medium: Memory
          name: mysys-shm
